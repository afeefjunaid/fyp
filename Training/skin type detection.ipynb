{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Noisy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_exts = ['jpg', 'jpeg', 'png', 'bmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import imghdr\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "\n",
    "data_dir = '../Preprocessing/Processed_Images'\n",
    "\n",
    "for image_class in os.listdir(data_dir):\n",
    "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "        image_path = os.path.join(data_dir, image_class, image)\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in img_exts:\n",
    "                print(image_path)    # print the path of the image with unknown extension\n",
    "                os.remove(image_path)\n",
    "        except Exception as e:\n",
    "            print(\"Issue with image:\" .format(image_path))\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [16,32]\n",
    "for size in batch_size:\n",
    "   data = tf.keras.utils.image_dataset_from_directory('../Preprocessing/Processed_Images', batch_size=size, image_size=(256, 256), shuffle=True) #Data pipeline\n",
    "\n",
    "class_names = data.class_names #get the class names\n",
    "print(class_names)\n",
    "\n",
    "data_iterator = data.as_numpy_iterator() #allows us to access Data pipeline\n",
    "\n",
    "batch = data_iterator.next() #get the next batch of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape # shape of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1] # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "rows = 4\n",
    "cols = 8\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 12))\n",
    "fig.suptitle('Sample Images with Class Labels', fontsize=16)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Display images in grid\n",
    "for idx, (image, label) in enumerate(zip(batch[0][:32], batch[1][:32])):\n",
    "    # Get class name from label index\n",
    "    class_name = class_names[label]\n",
    "    \n",
    "    # Display image\n",
    "    axes[idx].imshow(image.astype(\"uint8\"))\n",
    "    axes[idx].set_title(f'{class_name}', fontsize=8)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "# Add color-coded legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=f'C{i}', markersize=10, \n",
    "                             label=name) for i, name in enumerate(class_names)]\n",
    "fig.legend(handles=legend_elements, loc='center right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)  # Make room for legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x,y : ((x/255),y)) #normalizing the data  \n",
    "scaled_iterator = data.as_numpy_iterator() #allows us to access Data pipeline\n",
    "batch = scaled_iterator.next() #get the next batch of data\n",
    "\n",
    "batch[0].max() #max value in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 0.7) #70% of the data for training\n",
    "val_size = int(len(data) * 0.2) #20% of the data for validation\n",
    "test_size = int(len(data) * 0.1) #10% of the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.take(train_size) #take the first 70% of the data for training\n",
    "val = data.skip(train_size).take(val_size) #skip the first 70% and take the next 20% for validation\n",
    "test = data.skip(train_size + val_size).take(test_size) #skip the first 90% and take the next 10% for testing\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomTranslation, RandomContrast, RandomCrop\n",
    "# filter size = 3x3\n",
    "# input shape = 256x256x3\n",
    "# stride = 1\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = batch[0].shape[0]\n",
    "model = Sequential() #initialize the model\n",
    "\n",
    "\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    RandomFlip(\"horizontal_and_vertical\"),         # Flip images both horizontally and vertically\n",
    "    RandomRotation(0.4),                           # Rotate images up to 40% in both directions\n",
    "    RandomZoom(height_factor=(-0.2, 0.2),          # Random zoom in/out\n",
    "               width_factor=(-0.2, 0.2)),\n",
    "    RandomTranslation(height_factor=0.2,           # Translate images up to 20% in height\n",
    "                      width_factor=0.2),           # Translate images up to 20% in width\n",
    "    RandomContrast(0.2),                           # Adjust contrast randomly\n",
    "    RandomCrop(IMAGE_SIZE - 20, IMAGE_SIZE - 20),  # Crop random parts of the image\n",
    "    tf.keras.layers.Resizing(IMAGE_SIZE, IMAGE_SIZE)  # Resize back to target size\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking what is the expected dimension order for channel\n",
    "from tensorflow.keras import backend as k\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "batch_input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "chanDim = -1\n",
    "if k.image_data_format() == \"channels_first\":\n",
    "    input_shape = (CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    batch_input_shape = (BATCH_SIZE, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    chanDim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "model_dir = '../saved_models'\n",
    "log_dir = os.path.join(model_dir, 'logs')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(model_dir, 'best_model.keras'),\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        verbose=1\n",
    "    ),\n",
    "    TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        write_graph=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(model_dir, 'final_model.keras')\n",
    "model.save(final_model_path)\n",
    "print(f\"Model saved to {final_model_path}\")\n",
    "from matplotlib import pyplot as plt\n",
    "rows = 4\n",
    "cols = 8\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 12))\n",
    "fig.suptitle('Sample Images with Class Labels', fontsize=16)\n",
    "\n",
    "# Flatten axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Display images in grid\n",
    "for idx, (image, label) in enumerate(zip(batch[0][:32], batch[1][:32])):\n",
    "    # Get class name from label index\n",
    "    class_name = class_names[label]\n",
    "    \n",
    "    # Display image\n",
    "    axes[idx].imshow(image.astype(\"uint8\"))\n",
    "    axes[idx].set_title(f'{class_name}', fontsize=8)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "# Add color-coded legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=f'C{i}', markersize=10, \n",
    "                             label=name) for i, name in enumerate(class_names)]\n",
    "fig.legend(handles=legend_elements, loc='center right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)  # Make room for legend\n",
    "plt.show()\n",
    "\n",
    "print(train_size, val_size, test_size)\n",
    "\n",
    "train = data.take(train_size) #take the first 70% of the data for training\n",
    "val = data.skip(train_size).take(val_size) #skip the first 70% and take the next 20% for validation\n",
    "test = data.skip(train_size + val_size).take(test_size) #skip the first 90% and take the next 10% for testing\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import imghdr\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "img_exts = ['jpg', 'jpeg', 'png', 'bmp']\n",
    "data_dir = 'skinType'\n",
    "\n",
    "for image_class in os.listdir(data_dir):\n",
    "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "        image_path = os.path.join(data_dir, image_class, image)\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in img_exts:\n",
    "                print(image_path)    # print the path of the image with unknown extension\n",
    "                os.remove(image_path)\n",
    "        except Exception as e:\n",
    "            print(\"Issue with image:\" .format(image_path))\n",
    "batch_size = [16,32]\n",
    "for size in batch_size:\n",
    "   data = tf.keras.utils.image_dataset_from_directory('skinType', batch_size=size, image_size=(256, 256), shuffle=True) #Data pipeline\n",
    "\n",
    "class_names = data.class_names #get the class names\n",
    "print(class_names)\n",
    "\n",
    "data_iterator = data.as_numpy_iterator() #allows us to access Data pipeline\n",
    "\n",
    "batch = data_iterator.next() #get the next batch of data\n",
    "\n",
    "batch[0].shape # shape of the batch\n",
    "       \n",
    "batch[1] # labels\n",
    "\n",
    "data = data.map(lambda x,y : ((x/255),y)) #normalizing the data  \n",
    "scaled_iterator = data.as_numpy_iterator() #allows us to access Data pipeline\n",
    "batch = scaled_iterator.next() #get the next batch of data\n",
    "\n",
    "batch[0].max() #max value in the batch\n",
    "\n",
    "len(data)\n",
    "\n",
    "train_size = int(len(data) * 0.7) #70% of the data for training\n",
    "val_size = int(len(data) * 0.2) #20% of the data for validation\n",
    "test_size = int(len(data) * 0.1) #10% of the data for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from vit_pytorch import ViT\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, mean_squared_error\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lamb import Lamb\n",
    "from torch.optim import AdamW\n",
    "\n",
    "#unprocessed Data\n",
    "# dataset_path = '../skinType' \n",
    "\n",
    "#preprocessed Data\n",
    "dataset_path = '../Preprocessing/Processed_Images_FaceCrops_256' \n",
    "\n",
    "# Define hyperparameters\n",
    "batch_sizes = [ 8, 32,64]\n",
    "learning_rates =[0.01, 0.001]\n",
    "optimizers_list = [ 'LAMB', 'AdamW']\n",
    "total_epochs = 2\n",
    "num_classes = 3  # Dry, Normal, Oily skin types\n",
    "steps_per_epoch = 20 \n",
    "# Load Dataset\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(p=0.5),\n",
    "#     transforms.RandomVerticalFlip(p=0.5),\n",
    "#     transforms.RandomRotation(degrees=45),\n",
    "#     transforms.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6),\n",
    "#     transforms.RandomAffine(degrees=10, translate=(0.2, 0.2)), \n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "      transforms.Resize((256, 256)),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "# Dataset Path\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.20 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "print(f\"train size {train_size}\")\n",
    "print(f\"Val size {val_size}\")\n",
    "print(f\"Test Size {test_size}\")\n",
    "\n",
    "# print(f\"train Dataset {train_dataset}\")\n",
    "# print(f\"Val Dataset {val_dataset}\")\n",
    "# print(f\"Test Dataset {test_dataset}\")\n",
    "\n",
    "# Function to initialize the model\n",
    "def create_vit_model():    \n",
    "    model = ViT(\n",
    "        image_size=256,\n",
    "        patch_size=16,\n",
    "        num_classes=num_classes,  \n",
    "        dim=512,                      \n",
    "        heads=8,  \n",
    "        depth=8,              \n",
    "        mlp_dim=512,          \n",
    "        dropout=0.4,           \n",
    "        emb_dropout=0.4        \n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Optimizer choices\n",
    "def get_optimizer(optimizer_name, model_params, lr,weight_decay=0.001):\n",
    "    if optimizer_name == 'AdamW':\n",
    "        return optim.AdamW(model_params, lr=lr,weight_decay=weight_decay)\n",
    "    elif optimizer_name=='LAMB':\n",
    "        return Lamb(model_params, lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    all_probs = []\n",
    "\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Record predictions\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.detach().cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    try:\n",
    "        unique_classes = np.unique(all_labels)\n",
    "        if len(unique_classes) == num_classes:\n",
    "            roc_auc = roc_auc_score(all_labels, np.array(all_probs), multi_class='ovr', average='weighted')\n",
    "        else:\n",
    "            roc_auc = None\n",
    "    except ValueError:\n",
    "        roc_auc = None \n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
    "\n",
    "    return running_loss / len(loader), accuracy, precision, recall, f1, roc_auc, rmse  \n",
    "\n",
    "# Validation and Test Function\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    all_probs = [] \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    try:\n",
    "       \n",
    "        unique_classes = np.unique(all_labels)\n",
    "        if len(unique_classes) == num_classes:\n",
    "            roc_auc = roc_auc_score(all_labels, np.array(all_probs), multi_class='ovr', average='weighted')\n",
    "        else:\n",
    "            print(\"Warning: Not all classes are present in the evaluation set. Skipping ROC-AUC calculation.\")\n",
    "            roc_auc = None\n",
    "    except ValueError:\n",
    "        roc_auc = None  \n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
    "    \n",
    "    return running_loss / len(loader), accuracy, precision, recall, f1, roc_auc, rmse \n",
    "\n",
    "\n",
    "# Main training loop\n",
    "def train_and_evaluate(batch_size, lr, optimizer_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Model, Optimizer, Criterion\n",
    "    model = create_vit_model().to(device)\n",
    "    optimizer = get_optimizer(optimizer_name, model.parameters(), lr)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    # Training\n",
    "    for epoch in range(total_epochs):\n",
    "        # print(\"\\n\\n-------------------------Checking Weights in Each Iteration----------------------------\")\n",
    "        # print(\"Initial Weights:\")\n",
    "        # print_weights(model)\n",
    "        # print(\"\\n\\n-------------------------Checking Weights in Each Iteration-----------------------------\")\n",
    "  \n",
    "        train_loss, train_acc, train_precision, train_recall, train_f1, train_roc_auc, train_rmse = train_model(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        val_loss, val_acc, val_precision, val_recall, val_f1, val_roc_auc, val_rmse = evaluate_model(model, val_loader, criterion, device)\n",
    "        test_loss, test_acc, test_precision, test_recall, test_f1, test_roc_auc, test_rmse = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step()\n",
    "        # scheduler.step(val_loss)\n",
    "        print(\"----------Values After Training-----------\")\n",
    "        print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\",\n",
    "              f\"\\nOptimizer: {optimizer_name} \\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                   f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train  F1: {train_f1:.4f},\"\n",
    "                   f\"Train ROC AUC: {train_roc_auc:.4f}, Train RMSE: {train_rmse:.4f}\")\n",
    "        \n",
    "        print(\"\\n\\n-----------Values After Validation-----------\")\n",
    "        print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "              f\"\\nOptimizer: {optimizer_name} \\nVal Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "                    f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}, Val ROC AUC: {val_roc_auc:.4f}, \"\n",
    "                    f\"Val RMSE: {val_rmse:.4f}\")\n",
    "        \n",
    "        print(\"\\n\\n-----------Values After Testing-----------\")\n",
    "        print(f\"\\nEpoch: [{epoch+1}/{total_epochs}] \\nBatch Size: {batch_size} \\nLearning Rate: {lr}\"\n",
    "              f\"\\nOptimizer: {optimizer_name} Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, \"\n",
    "                    f\"Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "                \n",
    "        \n",
    "       \n",
    "        \n",
    "        #Saving Training data\n",
    "        overall_result = {\n",
    "            'Epoch': epoch + 1,\n",
    "            'Batch Size': batch_size,\n",
    "            'Learning Rate': lr,\n",
    "            'Optimizer': optimizer_name,\n",
    "            \n",
    "            'Train Loss':round(train_loss, 4),\n",
    "            'Test Loss':round(test_loss, 4),\n",
    "            'Val Loss':round(val_loss, 4),\n",
    "            \n",
    "            'Train Acc': round(train_acc, 4),\n",
    "            'Test Acc': round(test_acc, 4),\n",
    "            'Val Acc': round(val_acc, 4),\n",
    "            \n",
    "            \n",
    "            'Train Precision': round(train_precision, 4),\n",
    "            'Test Precision': round(test_precision, 4),\n",
    "            'Val Precision': round(val_precision, 4),\n",
    "            \n",
    "            'Train Recall': round(train_recall, 4),\n",
    "            'Test Recall': round(test_recall, 4),\n",
    "            'Val Recall': round(val_recall, 4),\n",
    "            \n",
    "            'Train F1 Score': round(train_f1, 4),\n",
    "            'Test F1 Score': round(test_f1, 4),\n",
    "            'Val F1 Score': round(val_f1, 4),\n",
    "            \n",
    "            'Train ROC AUC': round(train_roc_auc, 4) if train_roc_auc is not None else None,\n",
    "            'Test ROC AUC': round(test_roc_auc, 4) if test_roc_auc is not None else None,\n",
    "            'Val ROC AUC': round(val_roc_auc, 4) if val_roc_auc is not None else None,\n",
    "            \n",
    "            'Train RMSE': round(train_rmse, 4),\n",
    "            'Test RMSE': round(test_rmse, 4),\n",
    "            'Val RMSE': round(val_rmse, 4)\n",
    "        }\n",
    "        \n",
    "        # Append to CSV\n",
    "        overall_result_file = 'overall_result.csv'\n",
    "        \n",
    "        if not os.path.isfile(overall_result_file):\n",
    "            pd.DataFrame([overall_result]).to_csv(overall_result_file, index=False)        \n",
    "        else:\n",
    "            pd.DataFrame([overall_result]).to_csv(overall_result_file, mode='a', index=False, header=False)\n",
    "            \n",
    "    # Save the trained model\n",
    "    os.makedirs('../saved_models', exist_ok=True)\n",
    "    model_save_path = f\"../saved_models/vit_model_bs{batch_size}_lr{lr}_{optimizer_name}.pth\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    # # Convert results into DataFrame\n",
    "    df_results = pd.read_csv('overall_result.csv')\n",
    "    # Set up the plots with a style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Plot Loss (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_results[['Epoch', 'Train Loss', 'Test Loss', 'Val Loss']], x='Epoch')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Test Loss', 'Val Loss'])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Accuracy (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_results[['Epoch', 'Train Acc', 'Test Acc', 'Val Acc']], x='Epoch')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train Acc', 'Test Acc', 'Val Acc'])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Precision (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_results[['Epoch', 'Train Precision', 'Test Precision', 'Val Precision']], x='Epoch')\n",
    "    plt.title('Precision Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(['Train Precision', 'Test Precision', 'Val Precision'])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Recall (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_results[['Epoch', 'Train Recall', 'Test Recall', 'Val Recall']], x='Epoch')\n",
    "    plt.title('Recall Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend(['Train Recall', 'Test Recall', 'Val Recall'])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot F1 Score (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_results[['Epoch', 'Train F1 Score', 'Test F1 Score', 'Val F1 Score']], x='Epoch')\n",
    "    plt.title('F1 Score Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend(['Train F1', 'Test F1', 'Val F1'])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC AUC (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_results[['Epoch', 'Train ROC AUC', 'Test ROC AUC', 'Val ROC AUC']], x='Epoch')\n",
    "    plt.title('ROC AUC Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ROC AUC')\n",
    "    plt.legend(['Train ROC AUC', 'Test ROC AUC', 'Val ROC AUC'])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot RMSE (Train, Test, Val)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_results[['Epoch', 'Train RMSE', 'Test RMSE', 'Val RMSE']], x='Epoch')\n",
    "    plt.title('RMSE Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend(['Train RMSE', 'Test RMSE', 'Val RMSE'])\n",
    "    plt.show()\n",
    "    # return test_acc, precision, recall, f1, roc_auc, rmse\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for optimizer_name in optimizers_list:\n",
    "                #test_metrics = \n",
    "                train_and_evaluate(batch_size, lr, optimizer_name)\n",
    "                # print(f\"Final Test Metrics with Batch Size {batch_size}, LR {lr}, Optimizer {optimizer_name}: {test_metrics}\")\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Plot Loss (Train, Test, Val)\u001b[39;00m\n\u001b[32m      9\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43msns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlineplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_results\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEpoch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTrain Loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTest Loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mVal Loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEpoch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mLoss Over Epochs\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mEpoch\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\seaborn\\relational.py:515\u001b[39m, in \u001b[36mlineplot\u001b[39m\u001b[34m(data, x, y, hue, size, style, units, weights, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m color = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcolor\u001b[39m\u001b[33m\"\u001b[39m, kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    513\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolor\u001b[39m\u001b[33m\"\u001b[39m] = _default_color(ax.plot, hue, color, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ax\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\seaborn\\relational.py:295\u001b[39m, in \u001b[36m_LinePlotter.plot\u001b[39m\u001b[34m(self, ax, kws)\u001b[39m\n\u001b[32m    291\u001b[39m     grouped = sub_data.groupby(orient, sort=\u001b[38;5;28mself\u001b[39m.sort)\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# Could pass as_index=False instead of reset_index,\u001b[39;00m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# but that fails on a corner case with older pandas.\u001b[39;00m\n\u001b[32m    294\u001b[39m     sub_data = (\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m         \u001b[43mgrouped\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43magg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgroupby_apply_include_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m         .reset_index()\n\u001b[32m    298\u001b[39m     )\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    300\u001b[39m     sub_data[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mother\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m] = np.nan\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1819\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1816\u001b[39m     f = func\n\u001b[32m   1818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_groups:\n\u001b[32m-> \u001b[39m\u001b[32m1819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_obj_with_exclusions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m   1822\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1850\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1851\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1852\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1857\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1858\u001b[39m ) -> NDFrameT:\n\u001b[32m   1859\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1860\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1861\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1883\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1884\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1886\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1887\u001b[39m         not_indexed_same = mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m    918\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    921\u001b[39m     mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1809\u001b[39m, in \u001b[36mGroupBy.apply.<locals>.f\u001b[39m\u001b[34m(g)\u001b[39m\n\u001b[32m   1807\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m   1808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf\u001b[39m(g):\n\u001b[32m-> \u001b[39m\u001b[32m1809\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\seaborn\\_statistics.py:486\u001b[39m, in \u001b[36mEstimateAggregator.__call__\u001b[39m\u001b[34m(self, data, var)\u001b[39m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, var):\n\u001b[32m    485\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Aggregate over `var` column of `data` with estimate and error interval.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m     vals = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimator):\n\u001b[32m    488\u001b[39m         \u001b[38;5;66;03m# You would think we could pass to vals.agg, and yet:\u001b[39;00m\n\u001b[32m    489\u001b[39m         \u001b[38;5;66;03m# https://github.com/mwaskom/seaborn/issues/2943\u001b[39;00m\n\u001b[32m    490\u001b[39m         estimate = \u001b[38;5;28mself\u001b[39m.estimator(vals)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shazi\\OneDrive\\Desktop\\VS Code\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'y'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAAICCAYAAAD1U+1lAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAINBJREFUeJzt3Xus13X9wPEXINeAGE2EIMWxCVFLUAFpYFQb6w+3nPOPaFi6gXYlL6G5nCFYWWAk/YaXBlkpsaaW1qgxq7VWC9FaWgijFoTJARME5e7h/Pb+9DpnHG55zvmeLwd8PLZvcj68v+d8Tnudw3mez+XbrampqSkAAACI7qd6BwAAALoKgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAADUIpAeeOCBuPrqq0+6ZufOnXHzzTfHhAkTYuLEiXHnnXfGvn37OvJhAQAAOsVZ7X3iI488Et/+9rfjkksuOem6OXPmVEH00EMPxe7du+PLX/5y7N27N77xjW+090MDAAB0jUDatm1bfOUrX4k1a9bEyJEjT7r2z3/+czz99NOxatWqGDVqVLVt/vz5MWvWrLjpppvinHPOaf+eAwAAnOpT7P72t79Fz54948knn4wLL7zwpGufeeaZOPvss1viqCin2XXr1i2effbZ9u0xAABAVzmC9KEPfah6vNmjTcOGDWu1rVevXjFo0KDYunVrtEc5KtXU1FRFGgAA8NZ16NCh6uDL+PHjT/01SG9GufaoBNHRevfuHQcOHGjX+yxxVB4HDx6swR4CAADUKZD69Olz3JApcdSvX792vc9y5Ki8z3L9U9++fWuwl3DiwN+0aZNZo9OZNerFrFEvZo162bhxY3Tv3v30CaShQ4fGU0891WpbiZtXX301hgwZ0qH3Xb7Y2htZ0BZmjXoxa9SLWaNezBqdrZxed1q9UGx57aOGhobYvHlzy7ZyV7vi4osv7swPDQAAcGoDqbGxMV5++eXYv39/9Xa5y91FF10UN954Yzz33HPxxz/+Me6444644oor3OIbAAA4swOp3JluypQp1eseNR/y+r//+78YMWJEfPKTn4wbbrghLrvsspg3b14tPywAAMCpvwbp7rvvbvV2CaENGza02vaOd7wjlixZ0pEPAwAAUBedeg0SAADA6UQgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEB7A+nw4cOxZMmSmDp1aowbNy5mz54dW7ZsOeH6V155JW6++ea49NJLY9KkSXHjjTfGtm3b2vphAQAAul4gLV26NFasWBELFiyIlStXVsE0a9asOHjw4HHX33DDDfHSSy/F9773vepR/vzZz362FvsOAABw6gKpRNDy5ctjzpw5MW3atBgzZkwsXrw4GhoaYvXq1ces3717dzz99NPVUaZ3v/vdMXbs2Ljuuuvi+eefj1dffbWWnwcAAEB9A2n9+vWxZ8+emDx5csu2gQMHVuGzdu3aY9b36dMn3va2t8VPf/rTeP3116vHE088Eeeff371PAAAgK7krLYsLkeKimHDhrXaPmTIkJa/O1KvXr3i7rvvjjvuuCMuueSS6NatW7X24Ycfju7dO3Z/iH379nXo+fBmZ8ys0dnMGvVi1qgXs0a9NDU1VY1xygKpechL+Bypd+/esWvXruPu8AsvvBDjx4+vrlNqbGysTsn7zGc+Ez/60Y+if//+7d7xTZs2tfu50BZmjXoxa9SLWaNezBr1cHSb1DWQyilzzdciNf+5OHDgQPTt2/eY9b/4xS+qo0W/+c1vWmLo/vvvjw9+8IPx6KOPxjXXXNPuHR85cuRxPybUSvmFQPnGbtbobGaNejFr1ItZo142btxY8/fZpkBqPrVu+/btce6557ZsL2+PHj36mPXPPPNMdb3RkUeK3v72t1fbNm/e3KEdL19s/fr169D7gDfDrFEvZo16MWvUi1mjs9X69LqiTRcClbvWldhZs2ZNqzvVrVu3LiZMmHDM+qFDh1YhVI4wNdu7d2+8+OKL1W8UAAAAupLubT2/b+bMmbFo0aL41a9+Vd3Vrrzwawmh6dOnV9cYvfzyy7F///5q/RVXXNHyWkhlbXncdNNN1TVLV155Zed8RgAAAO3U5lvJlddAuuqqq+L222+PGTNmRI8ePWLZsmXRs2fP2Lp1a0yZMiVWrVpVrS13rCsvKltu1vDJT34yrr322mpd2TZgwID27jMAAECnaNM1SEUJorlz51aPo40YMSI2bNjQatuoUaOqGzMAAAB0dR17MSIAAIAziEACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAACAJJAAAgCSQAAAA2htIhw8fjiVLlsTUqVNj3LhxMXv27NiyZcsJ1x86dCjuueeelvUzZ86MF154oa0fFgAAoOsF0tKlS2PFihWxYMGCWLlyZRVMs2bNioMHDx53/bx58+Lxxx+Pr33ta/HYY4/F4MGDq6h67bXXarH/AAAApyaQSgQtX7485syZE9OmTYsxY8bE4sWLo6GhIVavXn3M+nJkqUTRV7/61eoI0qhRo+Kuu+6KXr16xV//+tfafRYAAAD1DqT169fHnj17YvLkyS3bBg4cGGPHjo21a9ces/73v/99DBgwIC677LJW63/961+3eh8AAABdwVltWVyOFBXDhg1rtX3IkCEtf3ekf/7zn/Gud72rOrr04IMPxrZt26qY+tKXvlQdTeqIffv2dej58GZnzKzR2cwa9WLWqBezRr00NTVFt27dTl0gNQ95OUXuSL17945du3Yds/7111+PzZs3V9ct3XLLLdXRo/vuuy8+/vGPx6pVq+Id73hHu3d806ZN7X4utIVZo17MGvVi1qgXs0Y9HN0mdQ2kPn36tFyL1Pzn4sCBA9G3b99j3/lZZ1WRVK5Taj5iVP78gQ98IH7yk59UN3dor5EjRx73Y0KtlF8IlG/sZo3OZtaoF7NGvZg16mXjxo01f59tCqTmU+u2b98e5557bsv28vbo0aOPWT906NAqko48na6EVTnt7sUXX+zQjpcvtn79+nXofcCbYdaoF7NGvZg16sWs0dlqfXpdm2/SUO5a179//1izZk3Ltt27d8e6detiwoQJx6wv29544414/vnnW7bt37+/urvdeeed19F9BwAAOHVHkMr5feWFXhctWlS9ntHw4cNj4cKF1ZGi6dOnR2NjY+zYsaO6c105UnTJJZfE+9///rj11ltj/vz5MWjQoOpFZnv06BEf/ehHa/uZAAAA1PuFYstrIF111VVx++23x4wZM6rYWbZsWfTs2TO2bt0aU6ZMqW7A0Ow73/lOTJw4MT73uc9VzyvXJP3gBz+oAgsAAOC0PYJUlCCaO3du9TjaiBEjYsOGDa22lVPy5s2bVz0AAADOqCNIAAAAZyqBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAABJIAEAACSBBAAAkAQSAABAEkgAAADtDaTDhw/HkiVLYurUqTFu3LiYPXt2bNmy5U0998knn4zRo0fHiy++2NYPCwAA0PUCaenSpbFixYpYsGBBrFy5sgqmWbNmxcGDB0/6vH//+98xf/78juwrAABA1wmkEkHLly+POXPmxLRp02LMmDGxePHiaGhoiNWrV5/weSWi5s6dG+95z3tqsc8AAACnPpDWr18fe/bsicmTJ7dsGzhwYIwdOzbWrl17wufdf//9cejQobj++us7trcAAACd6Ky2LC5Hiophw4a12j5kyJCWvzvac889Vx11evTRR2Pbtm1RK/v27avZ+4KTzZhZo7OZNerFrFEvZo16aWpqim7dup26QGoe8l69erXa3rt379i1a9cx6/fu3Rtf/OIXq8fIkSNrGkibNm2q2fuCkzFr1ItZo17MGvVi1qiHo9ukroHUp0+flmuRmv9cHDhwIPr27XvM+rvuuivOP//8+NjHPha1VoLreB8TaqX8QqB8YzdrdDazRr2YNerFrFEvGzdurPn7bFMgNZ9at3379jj33HNbtpe3y+27j/bYY49VRTd+/Pjq7cbGxuq/l19+eXzqU5+qHu1Vvtj69evX7ufDm2XWqBezRr2YNerFrNHZan16XZsDqdy1rn///rFmzZqWQNq9e3esW7cuZs6cecz6o+9s95e//KW6m92DDz4YF1xwQUf3HQAA4NQFUjkaVEJo0aJFMXjw4Bg+fHgsXLgwhg4dGtOnT6+OEO3YsSMGDBhQnYJ33nnntXp+840c3vnOd8agQYNq+5kAAADU+4Viy2sgXXXVVXH77bfHjBkzokePHrFs2bLo2bNnbN26NaZMmRKrVq3q6H4BAAB07SNIRQmicppceRxtxIgRsWHDhhM+d9KkSSf9ewAAgNPqCBIAAMCZSiABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAASSABAAAkgQQAAJAEEgAAQBJIAAAA7Q2kw4cPx5IlS2Lq1Kkxbty4mD17dmzZsuWE6zdu3BjXXXddTJo0KSZPnhxz5syJl156qa0fFgAAoOsF0tKlS2PFihWxYMGCWLlyZRVMs2bNioMHDx6zdufOnXHttddGnz594oc//GF897vfjR07dlTrDxw4UKvPAQAAoP6BVCJo+fLl1VGgadOmxZgxY2Lx4sXR0NAQq1evPmb9U089FXv37o1vfvObccEFF8R73/veWLhwYfzjH/+IP/3pT7X5DAAAAGrkrLYsXr9+fezZs6c6Va7ZwIEDY+zYsbF27dq4/PLLW60v68oRp3IEqVn37v9tst27d3dox/ft29eh58ObnTGzRmcza9SLWaNezBr10tTUFN26dTt1gVSOFBXDhg1rtX3IkCEtf3ekESNGVI8jPfjgg1UwTZgwITpi06ZNHXo+vFlmjXoxa9SLWaNezBr10KtXr1MXSM2/BTh6J3r37h27du36n88v1yE9/PDDcfvtt8fgwYOjI0aOHBl9+/bt0PuA/zXv5Ru7WaOzmTXqxaxRL2aNeik3hKu1NgVS86ly5VqkI0+bKzdcONnwl0Nf9957b9x3333x6U9/Oq6++uroqPLx+vXr1+H3A/+LWaNezBr1YtaoF7NGZ6v16XVtvklD86l127dvb7W9vH3OOecc9zmHDh2KuXPnxv333x+33XZb3HDDDR3ZXwAAgE7TpkAqd63r379/rFmzpmVbudnCunXrTnhN0S233BK//OUv45577olrrrmm43sMAADQSdp0il259mjmzJmxaNGi6hqi4cOHV7ftHjp0aEyfPj0aGxur1zkaMGBAdQre448/HqtWraoiaeLEifHyyy+3vK/mNQAAAKftC8WW10C66qqrqhstzJgxI3r06BHLli2Lnj17xtatW2PKlClVFBU///nPq/+W10Eq2498NK8BAAA4LY8gFSWIyjVF5XG0ckvvDRs2tLxdXlQWAADgjD2CBAAAcKYSSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAACQBBIAAEASSAAAAEkgAQAAJIEEAADQ3kA6fPhwLFmyJKZOnRrjxo2L2bNnx5YtW064fufOnXHzzTfHhAkTYuLEiXHnnXfGvn372vphAQAAul4gLV26NFasWBELFiyIlStXVsE0a9asOHjw4HHXz5kzJzZv3hwPPfRQ3HvvvfHb3/425s2bV4t9BwAAOHWBVCJo+fLlVfRMmzYtxowZE4sXL46GhoZYvXr1Mev//Oc/x9NPPx3f+MY34j3veU9Mnjw55s+fH0888URs27atlp8HAABAfQNp/fr1sWfPnip0mg0cODDGjh0ba9euPWb9M888E2effXaMGjWqZVs5za5bt27x7LPPdnTfAQAAauqstiwuR4qKYcOGtdo+ZMiQlr87UjlKdPTaXr16xaBBg2Lr1q3t2uFDhw5V/924cWMVWtBZmpqaqv+aNTqbWaNezBr1Ytaol9IGtZ6xNgVS880VSuQcqXfv3rFr167jrj96bfP6AwcOtH1vI1r+D+je3Q346Fxl1o43v1BrZo16MWvUi1mjnrN2SgOpT58+LdciNf+5KLHTt2/f464/3s0byvp+/fq1a4fHjx/frucBAAD8L206DNN8utz27dtbbS9vn3POOcesHzp06DFrSzC9+uqr1Wl5AAAAp20glbvW9e/fP9asWdOybffu3bFu3brqdY6OVraVa5PKbb6blbvaFRdffHHH9hwAAKDG2nSKXTmXdObMmbFo0aIYPHhwDB8+PBYuXFgdKZo+fXo0NjbGjh07YsCAAdXpdRdeeGFcdNFFceONN1avfbR3796444474oorrjjuEScAAIBTqVtT821G3qQSQd/61rfi8ccfj/3791dHiUr0jBgxIl588cX48Ic/HF//+tfjyiuvrNa/8sorceedd8bvfve76uYMH/nIR+K2226r/gwAAHBaBxIAAMCZyr2yAQAAkkACAABIAgkAACAJJAAAgCSQAAAAkkACAABIAgkAAKCrBtLhw4djyZIlMXXq1Bg3blzMnj07tmzZcsL1O3fujJtvvrl6wdqJEydWL0q7b9++uu4zp6e2ztrGjRvjuuuui0mTJsXkyZNjzpw58dJLL9V1n3lrzNqRnnzyyRg9enT1QtxQ61k7dOhQ3HPPPS3rZ86cGS+88EJd95m3xqy98sor1c9rl156afXv6I033hjbtm2r6z5z+nvggQfi6quvPumaWrRBlwukpUuXxooVK2LBggWxcuXK6gtw1qxZcfDgweOuLz+kbt68OR566KG4995747e//W3Mmzev7vvN6acts1a+2K699tro06dP/PCHP4zvfve7sWPHjmr9gQMHTsn+c/po6/e1Zv/+979j/vz5ddtP3nqzVv69fPzxx+NrX/taPPbYYzF48ODqB93XXnut7vvOmT1rN9xwQ/VLxe9973vVo/z5s5/9bN33m9PXI488Et/+9rf/57qatEFTF3LgwIGm8ePHNz3yyCMt23bt2tX0vve9r+lnP/vZMev/9Kc/NV1wwQVNf//731u2/e53v2saPXp0U0NDQ932m9NPW2ftxz/+cbV+3759Ldteeumlav7+8Ic/1G2/OfNnrVljY2PTjBkzmj7xiU9Uc7Zly5Y67TFvlVn717/+Vf17+Zvf/KbV+g9+8IO+r1HTWSt/V76P/epXv2rZ9tRTT1Xbdu7cWbf95vTU0NDQdP311zeNGzeu6SMf+UjTzJkzT7i2Vm3QpY4grV+/Pvbs2VOdvtRs4MCBMXbs2Fi7du0x65955pk4++yzY9SoUS3byqG0bt26xbPPPlu3/eb009ZZK+vKb8vKEaRm3bv/98tn9+7dddpr3gqz1uz++++vTn+6/vrr67SnvNVm7fe//30MGDAgLrvsslbrf/3rX7d6H9DRWSv/dr7tbW+Ln/70p/H6669XjyeeeCLOP//86nlwMn/729+iZ8+e1SnnF1544UnX1qoNzooupKGhofrvsGHDWm0fMmRIy98dqZy7evTaXr16xaBBg2Lr1q2dvLeczto6ayNGjKgeR3rwwQerb/rlHFeo1awVzz33XCxfvjweffRR5+jTabP2z3/+M971rnfF6tWrq+9nZdbKD7hf+tKXWv1wAR2dtfKz2d133x133HFHXHLJJdUPq2Xtww8/3PLLRjiRD33oQ9XjzahVG3SpqWy+gKp8Ikfq3bv3ca/zKOuPXnuy9dDeWTtauQ6pfGP/4he/WJ2zD7Watb1791ZzVR4jR46s237y1pu18lv8cp5+OTp+0003xX333RdnnXVWfPzjH68uqIdazVpTU1N184/x48dX15F8//vfj3e+853xmc98pppDqJVatUGXCqTm05eOvsCvfEJ9+/Y97vrjXQxY1vfr168T95TTXVtn7chv8uUCwbvuuis+/elP/887qUBbZ63MVjnt5GMf+1jd9pG35qyVGCo/nC5evDimTJkS73vf+6o/Fz/5yU/qtNe8FWbtF7/4RfVLxYULF8bFF19cnfJUTiMuN6IpR8qhVmrVBl0qkJoPiW3fvr3V9vL2Oeecc8z6oUOHHrO2/J/y6quvVoduoVazVpTrQebOnVt9U7/tttuqO/JArWet3EnsD3/4Q/Wb1vIodxQrLr/88mr2oJb/hpZIOvJ0uvLDRTntzm3lqeWsletCyi9++vfv37Lt7W9/e7WtHMWEWqlVG3SpQBozZkz1xbNmzZqWbeUC+HXr1h33Oo+yrZzreuQX19NPP139t/yGAmo1a8Utt9wSv/zlL6vXDLnmmmvquLe8lWatXA/y85//vLqYuTzKEaWiXCPiqBK1/jf0jTfeiOeff75l2/79+6vXsjnvvPPqtt+c+bNWfmgtP6sdeYpTOZ24hLhTiamlWrVBl7pJQzlnsLxI3aJFi6rrOoYPH14dji1fWNOnT4/GxsbqtWfKXXfKb7nKnSwuuuii6sXGyv3NyxdbuQDwiiuuOOFRAGjPrJXXCVm1alUVSeXUgJdffrnlfTWvgVrM2tE/mDZf8FzO1y8XmUKtZq1cLP/+978/br311ur1tsp8lRf+7NGjR3z0ox891Z8OZ9CslZ/Lli1bVp158YUvfKF6H+V09XJdyJVXXnmqPx1OY42d1QZNXcwbb7zR9M1vfrPp0ksvre53Pnv27JbX/yj/Lfc2f+yxx1rW/+c//2n6/Oc/X62dNGlS01e+8pWm/fv3n8LPgNNFW2bt2muvrd4+3uPIeYRafF870h//+Eevg0Snzdprr71W/btZ/v288MILq+91GzduPIWfAWfqrJXXpSmvZTNx4sTqOZ/73Od8X6PNbr311lavg9RZbdCt/E9nlh0AAMDpoktdgwQAAHAqCSQAAIAkkAAAAJJAAgAASAIJAAAgCSQAAIAkkAAAAJJAAgAASAIJAAAgCSQAAIAkkAAAAOK//h8kS9DiYDUjOgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_results = pd.read_csv('overall_result.csv')\n",
    "# Set up the plots with a style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Plot Loss (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_results[['Epoch', 'Train Loss', 'Test Loss', 'Val Loss']], x='Epoch')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train Loss', 'Test Loss', 'Val Loss'])\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_results[['Epoch', 'Train Acc', 'Test Acc', 'Val Acc']], x='Epoch')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train Acc', 'Test Acc', 'Val Acc'])\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_results[['Epoch', 'Train Precision', 'Test Precision', 'Val Precision']], x='Epoch')\n",
    "plt.title('Precision Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(['Train Precision', 'Test Precision', 'Val Precision'])\n",
    "plt.show()\n",
    "\n",
    "# Plot Recall (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_results[['Epoch', 'Train Recall', 'Test Recall', 'Val Recall']], x='Epoch')\n",
    "plt.title('Recall Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend(['Train Recall', 'Test Recall', 'Val Recall'])\n",
    "plt.show()\n",
    "\n",
    "# Plot F1 Score (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_results[['Epoch', 'Train F1 Score', 'Test F1 Score', 'Val F1 Score']], x='Epoch')\n",
    "plt.title('F1 Score Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(['Train F1', 'Test F1', 'Val F1'])\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC AUC (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_results[['Epoch', 'Train ROC AUC', 'Test ROC AUC', 'Val ROC AUC']], x='Epoch')\n",
    "plt.title('ROC AUC Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend(['Train ROC AUC', 'Test ROC AUC', 'Val ROC AUC'])\n",
    "plt.show()\n",
    "\n",
    "# Plot RMSE (Train, Test, Val)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_results[['Epoch', 'Train RMSE', 'Test RMSE', 'Val RMSE']], x='Epoch')\n",
    "plt.title('RMSE Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend(['Train RMSE', 'Test RMSE', 'Val RMSE'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
